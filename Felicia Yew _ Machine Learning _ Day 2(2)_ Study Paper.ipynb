{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing exercises: find 10 papers that have applied kNN, Decision Tree and Naive Bayes to solve their problems. For each paper, explain the problems they are solving, the techniques and the data that were used in the paper. Write this in your Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting service industry performance using decision tree analysis\n",
    "\n",
    "1. Problems\n",
    "\n",
    "Lack of concensus on the impact of information and communication technologies (ICTs) at the industry level of analysis, and insufficient attention given to service industries. \n",
    "\n",
    "2. Techniques:\n",
    "\n",
    "Chi-Square Automatic Interaction Detector (CHAID) decision tree as a predictive model to identify which variables predict service industry sales revenue growth, a proxy for industry performance. Decision trees are constructed as a set of decision rules, laid-out visually as a tree. Each branch has several nodes with if-then rules that take us through partitions from the root node to the target variable, based on a series of conditional probabilities. A split on each branch represents a subset of records based on values of an independent variable or predictor that leads to an outcome or value of the dependent variable, which is represented by leaf or terminal nodes (Osei-Bryson & Ngwenyama, 2011).\n",
    "\n",
    "3. Data: \n",
    "\n",
    "Data from the World Bank’s World Enterprise Survey (The World Bank, 2015). The data comprises industry aggregates of\n",
    "company-level data from 135 countries, from 2006 to 2015. It is organized\n",
    "into 12 topics: corruption, crime, finance, company characteristics,\n",
    "gender, informality, infrastructure, innovation and technology,\n",
    "performance, regulation and taxes, trade, and workforce.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Naive Bayes to Detect Spammy Names in Social Networks\n",
    "\n",
    "1. Problems:\n",
    "\n",
    "Detecting and removing accounts on the basis of their spammy names in order to improve the site experience for real members and prevent further abusive activity. Finding fake names acted as complementary strategy to existing spam-detection methods and can be used as part of a co-training process to create a broad model for fake accounts. \n",
    "\n",
    "2. Techniques:\n",
    "\n",
    "Name-spam detection algorithm using a Naive Bayes classifer. Using base feature set n-grams of letters that comprise these words for the name-spam detection. To improve further of missing features,for an n-gram that does not appear in the training set to replace with the two component (n - 1)-grams features.  \n",
    "It demonstrated the effectiveness of classifier by running the algorithm on LinkedIn members' name data, including both a labeled validation set and unlabeled data from production, and computing several performance metrics. Implementing the algorithm on production data cut the false positive rate by more than half as compared with LinkedIn's previous method of name spam detection. The algorithm runs on email usernames can supplement the name spam detection by correctly classifying some borderline cases.\n",
    "\n",
    "3. Data: \n",
    "\n",
    "A sample of roughly 60 million LinkedIn accounts that were either (a) in good standing as of June 14, 2013 or (b) had been agged by the LinkedIn\n",
    "Security team as fake and/or abusive at some point before that date. The accounts in (a) labeled as good and those in (b) as spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated web usage data mining and recommendation system using K-Nearest Neighbor (kNn) classification method\n",
    "\n",
    "1. Problems:\n",
    "\n",
    "Presentation of many choices of online websites to the client at a time which resulting more time consumption and task-consuming in finding the right product or information on the site. The study is done in order to reduce scalability problems, increase accuracy, reduces the problem of bottleneck caused by system computing load, and precise recommendation based on client's current navigation pattern. \n",
    "\n",
    "2. Techniques:\n",
    "\n",
    "Using automatic web usage data mining and recommendation system based on current user behavior through his/her click stream data on the newly developed Really Simple Syndication (RSS) reader website, in order to\n",
    "provide relevant information to the individual without explicitly asking for it. \n",
    "Pattern discovery and recommendation, which includes, path analysis, clustering, and associate rule using web usage data mining techniques and K-Nearest Neighbor algorithm classification. kNn classification technique used in order to observe and analyze user behavior pattern and click stream from the pre-process to web log stage and to recommend a unique set of object that satisfies the need of an active user, based on the users’ current click stream.\n",
    "\n",
    "3. Data: \n",
    "\n",
    "Data mart which is populated from raw users RSS address URL file of the RSS reader’s site that contains some basic fields needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost-sensitive payment card fraud detection based on dynamic random forest and k -nearest neighbors\n",
    "\n",
    "1. Problems:\n",
    "\n",
    "Payment card fraud leads to heavy annual financial losses around the world, which increase the need for improvements in the fraud detection systems used by banks and financial institutions. Developing a method that involves two stages of fraudulent payment card transaction detion by using suitable transactional features extraction. Behavioral changes can serve as the basis for fraud detection. \n",
    "\n",
    "2. Techniques:\n",
    "\n",
    "kNn algorithm is used in the first stage to determine the similarity between a new transaction and past transactions. second stage is incorporated using DRF algorithm in initial cost-insensitive fraud detection and minimum risk model in cost-sensitive fraud detection to prevent the occurence of false positives. \n",
    "\n",
    "3. Data: \n",
    "\n",
    "Real dataset obtained from a pri- vate bank. The dataset covered 18 months (March 2012–September 2013) of transactions; specifically 54,105 payment card transactions made using 415 different payment cards. A fraud label created by the bank was assigned to fraudulent transactions that were reported by the cardholders or discovered by the bank. These transactions accounted for 7% of the dataset. The rest of the data were chosen from legitimate transactions in the same period. The fraud types in dataset consist of lost and stolen cards, mail and telephone orders, and account takeover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrusion Detection using Naive Bayes Classifier with Feature Reduction\n",
    "\n",
    "1. Problems:\n",
    "\n",
    "Most of the intrusion detection approaches focused on the issues of feature selection or reduction which some features might be irrelevant and redundant that leads to lengthy detection process and degrades the performance of an intrusion detection system (IDS). \n",
    "\n",
    "The purpose of this study \n",
    "1)identify important reduced input features in building IDS that is computationally efficient and effective. \n",
    "2)investigate the performance of three standard feature selection methods using Correlation-based Feature Selection, Information Gain and Gain Ratio. \n",
    "\n",
    "2. Techniques:\n",
    "\n",
    "Using Feature Vitality Based Reduction Method, to identify important reduced input features. Application of  efficient classifier naive bayes on reduced datasets for intrusion detection. The vitality of feature is determined by considering three main performance criteria the classification accuracy, TPR and FPR of the system. \n",
    "\n",
    "The feature selection of is “leave-one-out” remove one feature from the original dataset, redo the experiment, then compare the new results with the original results. \n",
    "\n",
    "The Naïve Bayes classifier chosen with full training set and 10-fold cross validation for the testing purposes. In 10-fold cross-validation, the available data is randomly divided into 10 disjoint subsets of approximately equal size. One of the subsets is then used as the test set and the remaining 9 sets are used for building the classifier. The test set is then used to estimate the accuracy. This is done repeatedly 10 times so that each subset is used as a test subset once. The accuracy estimates is then the mean of the estimates for each of the classifiers. Cross-validation has been tested.\n",
    "\n",
    "3. Data: \n",
    "\n",
    "NSL-KDD labeled dataset suggested to solve some of the inherent problems of the KDD'99 data set. NSL-KDD dataset contains one type of normal\n",
    "data and 22 different types of attacks which falls into one of four categories. These are DoS, probe, R2L , and U2R. we extracted only 62,986 records out of 1,25,973 NSL-KDD dataset connections for training\n",
    "and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting copper prices by decision tree learning\n",
    "\n",
    "1. Problems:\n",
    "\n",
    "Forecasting the prices of metals is important in many aspects of economics and financial models for revenue evaluation, that acted basis of an effective payment regime using by resource policymakers. Therefore, utilizing machine-learning algorithm in predicting copper prices in both short-term (days) and long-term (years), with mean absolute percentage errors below 4%. \n",
    "\n",
    "2. Techniques:\n",
    "\n",
    "Decision tree algorithm to predict future copper price. \n",
    "Seven external factors that affect copper prices were used for constructing the independent variables; however, other factors and\n",
    "variables can be easily used in training the decision tree model. In\n",
    "addition, data from various days (i.e. parameter D) can be used as the input variables, thus extending the flexibility of current method. \n",
    "\n",
    "3. Data: \n",
    "\n",
    "Prices of gold, silver, copper, crude oil, natural gas, lean hogs, and coffee, as well as the Dow Jones indices from Jan 03, 2008 to Dec 01, 2015. All the data were downloaded from http://www.investing.com. Note that, as exclusion of data for weekends and holidays were missing in the raw data.  The processed data set contained prices from 2062 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow learning model for diagnosing neuro muscular disorder from splicing variants\n",
    "\n",
    "1. Problems:\n",
    "\n",
    "Diagnosing genetic neuromuscular disorder such as muscular dystrophy is complicated when the imperfection occurs while splicing. To predict the type of muscular dystrophy from the gene sequences by extracting the well-defined descriptors related to splicing mutations. An automatic model is built to classify the disease through pattern recognition techniques coded in python using scikit-learn framework.\n",
    "\n",
    "2. Techniques:\n",
    "\n",
    "Multi-class classification is worked out through data modeling of gene sequences. The synthetic mutational gene sequences are created, as the diseased gene sequences are not readily obtainable for this intricate disease. Positional cloning approach supports in generating disease gene sequences based on mutational information acquired from HGMD. SNP-, gene- and exon-based discriminative features are identified and used to train the model. The training data set with instances related to five categories of muscular dystrophy, that is, Duchenne muscular dystrophy,\n",
    "Becker’s muscular dystrophy, Emery–Dreifuss, Limb-girdle muscular dystrophy and Charcot Marie Tooth disease \n",
    "\n",
    "An eminent muscular dystrophy disease prediction model is built using supervised learning techniques in scikit-learn environment. The data frame is built with the extracted features as numpy array. The data are normalized by transforming the feature values into the range between 0 and 1 aid in scaling the input attributes for a model. Naïve Bayes, decision tree, K-nearest neighbor and SVM learned models are developed using python library framework in scikit-learn.\n",
    "\n",
    "Scoring parameters of the classifiers are evaluated using\n",
    "tenfold cross-validation using metric functions of scikit-learn\n",
    "library.\n",
    "\n",
    "3. Data: \n",
    "\n",
    "The cloned gene sequences are synthesized based on the mutation position and its location on the chromosome by using the positional cloning approach. The mutational information for splicing mutation is specified as IVS1-5 T > G indicates (IVS - intervening sequence or introns) first intron and five nucleotides before the consensus intron site AG, where the variant occurs in nucleotide G altered to T. IVS (+ve) denotes forward strand 3= – positive numbers from G of donor site invariant and IVS (+ve) denotes backward strand 5= – negative numbers starting from G of acceptor site.\n",
    "\n",
    "Various types of genes associated with the five types of neuromuscular disorder are studied. A review is made from 55 genes that are associated with five types of muscular dystrophy like DMD, BMD, EMD, LGMD and CMT. Several types of mutated sequences based on mutations like missense, non-sense, synonymous, insertion/duplication, deletion mutations and splicing mutations are collected. In each category of muscular dystrophy disease, 120 synthetic mutated gene sequences are generated and a corpus comprising 600 sequences for all five categories of muscular dystrophy is developed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing WO3 gas sensor selectivity using a set of pollutant detection classifiers\n",
    "\n",
    "1. Problems: \n",
    "\n",
    "Presence of cross-selectivity of gases in electronic nose. \n",
    "Improving the classification accuracy for gas discrimination. \n",
    "\n",
    "2. Techniques:\n",
    "\n",
    "Fusion approach of two classifiers: the first one that is a nonlinear model is the SVM classifier and the second one is a linear classifier the KNN model.\n",
    "\n",
    "An array of three gas metal oxide sensors was applied to identify three gases: ozone, ethanol and acetone. The sensors array coupled with feature extraction and pattern recognition methods, namely PCA, SVM and KNN were trained to classify the data clusters related to different gases. Moreover, the performance of the hybrid fusion classifiers by exploiting the new approach of data fusion of different classifiers was illustrated. This performance was proved through a comparison with that of the individual classification model.\n",
    "\n",
    "The data set extracted from proposed feature in the form of T12 x 3 (as shown in matrix T) is divided in two sub-data sets; the first one was the learning data set, and the second one was the test data sets. The model performance was estimated as the average performance over the n tests of the training data set using leave-one-out cross-validation\n",
    "\n",
    "3. Data: \n",
    "\n",
    "Two features were extracted from each sample. The first feature is the derivate; the second feature is the integral. We characterized samples (3 gases x 4 temperatures) and the size of the data matrix is (12 measures x 6 variables). The obtained data set extracted from the extracted\n",
    "features is introduced into the PCA process. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam filtering with Naive Bayes - Which Naive Bayes?\n",
    "\n",
    "1. Problems: \n",
    "\n",
    "To acknowledge the benefits and determine the effectiveness of five different versions of Naive Bayes, and compare them on six new, non-encoded datasets, that contain ham messages of particular Enron users and fresh spam messages.\n",
    "\n",
    "2. Techniques:\n",
    "\n",
    "Multivariate Bernoulli NB\n",
    "Multinomial NB, TF attributes\n",
    "Multinomial NB, Boolean attributes\n",
    "Multivariate Gauss NB\n",
    "Flexible Bayes\n",
    "\n",
    "3. Data: \n",
    "\n",
    "Enron-Spam datasets are available from http://www.iit.demokritos.gr/skel/i-config/ and http://www.aueb.gr/users/ion/publications.html in both raw and pre-processed form. Six, non-encoded individual dataset contains ham (non-spam) messages from a single user of the Enron corpus, to which fresh spam messages added with varying ham-spam ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machine and K-nearest neighbour for unbalanced fault detection\n",
    "\n",
    "1. Problems:\n",
    "\n",
    "Determine an appropriate approach for detecting unbalanced fault in rotating machine\n",
    "\n",
    "\n",
    "2. Techniques:\n",
    "\n",
    "Supervised learning method with KNN and SVM Classifier\n",
    "\n",
    "To fulfil this goal, a fault diagnosis approach based on signal\n",
    "processing, feature extraction and fault classification, was used. Vibration signals were acquired from a designed experimental system with three conditions, namely, no load, balanced load and unbalanced load. FFT technique was applied to transform the vibration signals from time-domain\n",
    "into frequency-domain. In total, 29 feature parameters were extracted from FFT amplitude of the signals. SVM and KNN were employed to classify the three different conditions. The performances of the two classifiers were obtained under different values of their parameter.\n",
    "\n",
    "3. Data: \n",
    "\n",
    "The data collection system consisted of a B&K NEXUS Conditioning Amplifier Type 2692 low-noise version with a high bandwidth and a 10 mV/ms2 output, and a data acquisition with a sampling frequency of 1 MHz.\n",
    "In this work, 20 kHz sampling frequency was used. Processed signals comprises a large set of data for each sample. 29 features were extracted from FFT amplitude of the vibration signals. Some of the used features are maximum, range, average, root mean square, standard deviation, variance, fifth central moment, sixth central moment, skewness, kurtosis, etc. Total of 1,044 samples for each condition (no load, balanced load and unbalanced load).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] B. Yeo and D. Grant, “Predicting Service Industry Performance Using Decision Tree Analysis,” Int. J. Inf. Manage., vol. 38, no. 1, pp. 288–300, 2018.\n",
    "\n",
    "[2] D. M. Freeman, “Using Naive Bayes to Detect Spammy Names in Social Networks,” AISec, pp. 3–12, 2013.\n",
    "\n",
    "[3] D. A. Adeniyi, Z. Wei, and Y. Yongquan, “Automated web usage data mining and recommendation system using K-Nearest Neighbor (KNN) classification method,” Appl. Comput. Informatics, vol. 12, no. 1, pp. 90–108, 2016.\n",
    "\n",
    "[4] S. Nami and M. Shajari, “Cost-Sensitive Payment Card Fraud Detection Based on Dynamic Random Forest and k-Nearest Neighbors,” Expert Syst. Appl., vol. 110, pp. 381–392, 2018.\n",
    "\n",
    "[5] S. Mukherjee and N. Sharma, “Intrusion Detection using Naive Bayes Classifier with Feature Reduction,” Procedia Technol., vol. 4, pp. 119–128, 2012.\n",
    "\n",
    "[6] C. Liu, Z. Hu, Y. Li, and S. Liu, “Forecasting Copper Prices by Decision Tree Learning,” Resour. Policy, vol. 52, pp. 427–434, 2017.\n",
    "\n",
    "[7] S. Kalimuthu and V. Vijayakumar, “Shallow Learning Model for Diagnosing Neuro Muscular Disorder from Splicing Variants,” World J. Eng., vol. 14, no. 4, pp. 329–336, 2017.\n",
    "\n",
    "[8] M. Othman, R. Faleh, S. Gomri, A. Kachouri, and K. Aguir, “Enhancing WO 3 Gas Sensor Selectivity Using a Set of Pollutant Detection Classifiers,” Sens. Rev., vol. 38, no. 1, pp. 65–73, 2017.\n",
    "\n",
    "[9] I. Androutsopoulos and V. Paliouras, GMetsis, “Spam Filtering with Naive Bayes-Which Naive Bayes?,” Ceas - Third Conf. Email Anti-Spam, p. 9, 2006.\n",
    "\n",
    "[10] A. Moosavian, H. Ahmadi, B. Sakhaei, and R. Labbafi, “Support Vector Machine and K-Nearest Neighbour for Unbalanced Fault Detection,” J. Qual. Maint. Eng., vol. 20, no. 1, pp. 65–75, 2014.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
